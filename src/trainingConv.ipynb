{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\parth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from visual_transformer import ConvNeXtModel\n",
    "from DIPutils import DIPTransform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataset path\n",
    "dataset_path = r\"C:\\Users\\parth\\Downloads\\Kindey-Stone-Dataset\\Kindey-Stone-Dataset\\Augmented\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations (resize, normalize, etc.)\n",
    "transform = transforms.Compose([\n",
    "    # DIPTransform(target_size=500),  # Apply DIP pipeline\n",
    "    # transforms.ToPILImage(),        # Convert back to PIL imagev\n",
    "    transforms.Resize((224, 224)),  # Resize to model input size\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 24821, Validation size: 5318, Test size: 5318\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset_path = dataset_path\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "# Define split sizes\n",
    "total_size = len(dataset)\n",
    "val_size = int(0.15 * total_size)  # 15% for validation\n",
    "test_size = int(0.15 * total_size)  # 15% for testing\n",
    "train_size = total_size - (val_size + test_size)  # Remaining 70% for training\n",
    "\n",
    "# Split dataset\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64 \n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train size: {len(train_ds)}, Validation size: {len(val_ds)}, Test size: {len(test_ds)}\")\n",
    "# also print the number of classes\n",
    "num_classes = len(dataset.classes)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNeXtModel(\n",
      "  (convnext): ConvNeXt(\n",
      "    (stem): Sequential(\n",
      "      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "    (stages): Sequential(\n",
      "      (0): ConvNeXtStage(\n",
      "        (downsample): Identity()\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (3): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (4): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (5): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (6): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (7): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (8): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm_pre): Identity()\n",
      "    (head): NormMlpClassifierHead(\n",
      "      (global_pool): SelectAdaptivePool2d(pool_type=, flatten=Identity())\n",
      "      (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (flatten): Identity()\n",
      "      (pre_logits): Identity()\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "      (fc): Identity()\n",
      "    )\n",
      "  )\n",
      "  (global_avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (dense1): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (classifier): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = ConvNeXtModel(input_shape=(224, 224, 3), num_classes=2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "def compute_accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return (preds == labels).float().mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/20]\n",
      "Train Loss: 0.4311, Train Accuracy: 0.8750\n",
      "Val Loss: 0.3630, Val Accuracy: 0.9489\n",
      "\n",
      "Epoch [2/20]\n",
      "Train Loss: 0.3557, Train Accuracy: 0.9561\n",
      "Val Loss: 0.3444, Val Accuracy: 0.9701\n",
      "\n",
      "Epoch [3/20]\n",
      "Train Loss: 0.3425, Train Accuracy: 0.9699\n",
      "Val Loss: 0.3450, Val Accuracy: 0.9677\n",
      "\n",
      "Epoch [4/20]\n",
      "Train Loss: 0.3353, Train Accuracy: 0.9774\n",
      "Val Loss: 0.3392, Val Accuracy: 0.9724\n",
      "\n",
      "Epoch [5/20]\n",
      "Train Loss: 0.3329, Train Accuracy: 0.9797\n",
      "Val Loss: 0.3514, Val Accuracy: 0.9592\n",
      "\n",
      "Epoch [6/20]\n",
      "Train Loss: 0.3308, Train Accuracy: 0.9821\n",
      "Val Loss: 0.3380, Val Accuracy: 0.9744\n",
      "\n",
      "Epoch [7/20]\n",
      "Train Loss: 0.3285, Train Accuracy: 0.9843\n",
      "Val Loss: 0.3368, Val Accuracy: 0.9771\n",
      "\n",
      "Epoch [8/20]\n",
      "Train Loss: 0.3255, Train Accuracy: 0.9875\n",
      "Val Loss: 0.3434, Val Accuracy: 0.9707\n",
      "\n",
      "Epoch [9/20]\n",
      "Train Loss: 0.3247, Train Accuracy: 0.9884\n",
      "Val Loss: 0.3312, Val Accuracy: 0.9812\n",
      "\n",
      "Epoch [10/20]\n",
      "Train Loss: 0.3277, Train Accuracy: 0.9852\n",
      "Val Loss: 0.3313, Val Accuracy: 0.9833\n",
      "\n",
      "Epoch [11/20]\n",
      "Train Loss: 0.3273, Train Accuracy: 0.9853\n",
      "Val Loss: 0.3341, Val Accuracy: 0.9803\n",
      "\n",
      "Epoch [12/20]\n",
      "Train Loss: 0.3239, Train Accuracy: 0.9891\n",
      "Val Loss: 0.3305, Val Accuracy: 0.9823\n",
      "\n",
      "Epoch [13/20]\n",
      "Train Loss: 0.3216, Train Accuracy: 0.9915\n",
      "Val Loss: 0.3273, Val Accuracy: 0.9853\n",
      "\n",
      "Epoch [14/20]\n",
      "Train Loss: 0.3223, Train Accuracy: 0.9907\n",
      "Val Loss: 0.3339, Val Accuracy: 0.9786\n",
      "\n",
      "Epoch [15/20]\n",
      "Train Loss: 0.3213, Train Accuracy: 0.9917\n",
      "Val Loss: 0.3478, Val Accuracy: 0.9646\n",
      "\n",
      "Epoch [16/20]\n",
      "Train Loss: 0.3237, Train Accuracy: 0.9892\n",
      "Val Loss: 0.3300, Val Accuracy: 0.9835\n",
      "\n",
      "Epoch [17/20]\n",
      "Train Loss: 0.3196, Train Accuracy: 0.9936\n",
      "Val Loss: 0.3318, Val Accuracy: 0.9814\n",
      "\n",
      "Epoch [18/20]\n",
      "Train Loss: 0.3217, Train Accuracy: 0.9913\n",
      "Val Loss: 0.3274, Val Accuracy: 0.9850\n",
      "\n",
      "Epoch [19/20]\n",
      "Train Loss: 0.3197, Train Accuracy: 0.9934\n",
      "Val Loss: 0.3315, Val Accuracy: 0.9814\n",
      "\n",
      "Epoch [20/20]\n",
      "Train Loss: 0.3215, Train Accuracy: 0.9917\n",
      "Val Loss: 0.3326, Val Accuracy: 0.9803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20  # Set number of epochs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for inputs, labels in train_loader:  # Assume train_loader is your DataLoader\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        correct += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct / total\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_correct, val_total = 0, 0\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients for validation\n",
    "        for val_inputs, val_labels in val_loader:  # Assume val_loader is your validation DataLoader\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "            val_outputs = model(val_inputs)\n",
    "            loss = criterion(val_outputs, val_labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_correct += (torch.argmax(val_outputs, dim=1) == val_labels).sum().item()\n",
    "            val_total += val_labels.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save teh model\n",
    "torch.save(model.state_dict(), 'model20conv.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Test Loss: 0.3377, Test Accuracy: 0.9744\n"
     ]
    }
   ],
   "source": [
    "# test teh model\n",
    "print(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "correct, total = 0, 0\n",
    "running_loss = 0.0\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "with torch.no_grad():  # No need to track gradients for validation\n",
    "    for test_inputs, test_labels in test_loader:  # Assume val_loader is your validation DataLoader\n",
    "        test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "        test_outputs = model(test_inputs)\n",
    "        loss = criterion(test_outputs, test_labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        correct += (torch.argmax(test_outputs, dim=1) == test_labels).sum().item()\n",
    "        total += test_labels.size(0)\n",
    "test_loss = running_loss / len(test_loader)\n",
    "test_accuracy = correct / total\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate model on test data and return metrics\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        test_loader: DataLoader with test data\n",
    "        device: 'cuda' or 'cpu'\n",
    "    Returns:\n",
    "        Dictionary containing metrics and confusion matrix\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')  # for 2 classes\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return {\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(cm):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.9553\n",
      "F1 Score: 0.9711\n",
      "Confusion Matrix:\n",
      "[[2894   29]\n",
      " [ 107 2288]]\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_model(model, test_loader, device)\n",
    "\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(metrics['confusion_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(metrics['confusion_matrix'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
